#!/usr/bin/env python3
"""
Script to generate comprehensive type hints for data transformation functions.
"""

import ast
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.models.pydantic_models import (
    ScrapingConfig, ScrapingJob, ScrapedData, ScrapingResult,
    JobStatus, ContentType, DataExportRequest
)


class TypeHintGenerator:
    """Generate type hints for data transformation functions."""
    
    def __init__(self):
        self.type_mappings = {
            'str': 'str',
            'int': 'int',
            'float': 'float',
            'bool': 'bool',
            'list': 'List[Any]',
            'dict': 'Dict[str, Any]',
            'datetime': 'datetime',
            'uuid': 'str',
            'optional_str': 'Optional[str]',
            'optional_int': 'Optional[int]',
            'optional_float': 'Optional[float]',
            'optional_bool': 'Optional[bool]',
            'optional_datetime': 'Optional[datetime]',
            'scraped_data_list': 'List[ScrapedData]',
            'job_list': 'List[ScrapingJob]',
            'content_dict': 'Dict[str, Any]',
            'metadata_dict': 'Dict[str, Any]',
            'ai_metadata_dict': 'Dict[str, Any]'
        }
    
    def generate_api_type_hints(self) -> str:
        """Generate type hints for API transformation functions."""
        return '''
"""
Type hints for API data transformation functions.
Auto-generated by generate_type_hints.py
"""

from typing import Dict, List, Any, Optional, Union
from datetime import datetime
from src.models.pydantic_models import ScrapedData, ScrapingJob, ScrapingResult

# API Response Transformation Functions

def transform_job_to_api_response(job: ScrapingJob) -> Dict[str, Any]:
    """Transform ScrapingJob model to API response format."""
    pass

def transform_scraped_data_to_api_response(data: ScrapedData, job_name: Optional[str] = None) -> Dict[str, Any]:
    """Transform ScrapedData model to API response format."""
    pass

def transform_job_list_to_api_response(jobs: List[ScrapingJob]) -> List[Dict[str, Any]]:
    """Transform list of ScrapingJob models to API response format."""
    pass

def transform_scraped_data_list_to_api_response(
    data_list: List[ScrapedData], 
    job_mapping: Optional[Dict[str, str]] = None
) -> List[Dict[str, Any]]:
    """Transform list of ScrapedData models to API response format."""
    pass

# Export Format Transformation Functions

def transform_data_to_csv_format(data_list: List[Dict[str, Any]]) -> List[Dict[str, str]]:
    """Transform API response data to CSV-compatible format."""
    pass

def transform_data_to_json_format(
    data_list: List[Dict[str, Any]], 
    include_metadata: bool = True
) -> Dict[str, Any]:
    """Transform API response data to JSON export format."""
    pass

def transform_data_to_excel_format(data_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Transform API response data to Excel-compatible format."""
    pass

# Data Validation Functions

def validate_api_response_structure(response_data: Dict[str, Any]) -> Tuple[bool, List[str]]:
    """Validate API response structure and return validation results."""
    pass

def validate_export_data_consistency(
    original_data: List[ScrapedData], 
    transformed_data: List[Dict[str, Any]]
) -> Tuple[bool, List[str]]:
    """Validate consistency between original and transformed data."""
    pass

def validate_field_types(data: Dict[str, Any], expected_types: Dict[str, type]) -> List[str]:
    """Validate field types in data dictionary."""
    pass

# Content Processing Functions

def extract_content_summary(content: Dict[str, Any]) -> str:
    """Extract summary from content dictionary."""
    pass

def calculate_content_metrics(content: Dict[str, Any]) -> Dict[str, Union[int, float]]:
    """Calculate metrics for content data."""
    pass

def normalize_ai_metadata(ai_metadata: Dict[str, Any]) -> Dict[str, Any]:
    """Normalize AI metadata to standard format."""
    pass

def merge_content_metadata(
    content_metadata: Dict[str, Any], 
    ai_metadata: Dict[str, Any]
) -> Dict[str, Any]:
    """Merge content and AI metadata dictionaries."""
    pass

# Utility Functions

def safe_get_nested_value(data: Dict[str, Any], keys: List[str], default: Any = None) -> Any:
    """Safely get nested value from dictionary."""
    pass

def format_datetime_for_api(dt: Optional[datetime]) -> Optional[str]:
    """Format datetime for API response."""
    pass

def parse_api_datetime(dt_str: Optional[str]) -> Optional[datetime]:
    """Parse datetime string from API."""
    pass

def sanitize_string_for_export(value: Any) -> str:
    """Sanitize value for export formats."""
    pass

def calculate_confidence_score(
    content_length: int, 
    ai_processed: bool, 
    validation_errors: List[str]
) -> float:
    """Calculate confidence score based on content metrics."""
    pass
'''
    
    def generate_pipeline_type_hints(self) -> str:
        """Generate type hints for pipeline transformation functions."""
        return '''
"""
Type hints for data pipeline transformation functions.
Auto-generated by generate_type_hints.py
"""

from typing import Dict, List, Any, Optional, Union, Tuple, AsyncGenerator
from datetime import datetime
from src.models.pydantic_models import ScrapedData, ScrapingJob, ScrapingResult, ScrapingConfig

# Pipeline Processing Functions

async def process_scraping_job(
    job: ScrapingJob, 
    config: Optional[ScrapingConfig] = None
) -> ScrapingResult:
    """Process a scraping job and return results."""
    pass

async def process_scraped_data_batch(
    data_batch: List[ScrapedData]
) -> List[ScrapedData]:
    """Process a batch of scraped data."""
    pass

def clean_scraped_content(content: Dict[str, Any]) -> Dict[str, Any]:
    """Clean and normalize scraped content."""
    pass

def validate_scraped_data(data: ScrapedData) -> Tuple[bool, List[str]]:
    """Validate scraped data and return validation results."""
    pass

# Content Extraction Functions

def extract_text_content(html_content: str) -> str:
    """Extract clean text from HTML content."""
    pass

def extract_structured_data(
    html_content: str, 
    selectors: Dict[str, str]
) -> Dict[str, Any]:
    """Extract structured data using CSS selectors."""
    pass

def extract_metadata_from_html(html_content: str, base_url: str) -> Dict[str, Any]:
    """Extract metadata from HTML content."""
    pass

def extract_links_from_content(
    html_content: str, 
    base_url: str, 
    filter_external: bool = True
) -> List[Dict[str, str]]:
    """Extract links from HTML content."""
    pass

# AI Processing Functions

async def process_content_with_ai(
    content: str, 
    content_type: str = "html"
) -> Dict[str, Any]:
    """Process content using AI and return analysis."""
    pass

def standardize_ai_response(ai_response: Dict[str, Any]) -> Dict[str, Any]:
    """Standardize AI response format."""
    pass

def calculate_ai_confidence_score(ai_metadata: Dict[str, Any]) -> float:
    """Calculate confidence score from AI metadata."""
    pass

# Data Quality Functions

def assess_data_quality(data: ScrapedData) -> float:
    """Assess overall data quality score."""
    pass

def identify_data_issues(data: ScrapedData) -> List[str]:
    """Identify potential data quality issues."""
    pass

def calculate_content_completeness(content: Dict[str, Any]) -> float:
    """Calculate content completeness score."""
    pass

# Batch Processing Functions

async def process_job_queue(
    job_queue: List[ScrapingJob]
) -> AsyncGenerator[ScrapingResult, None]:
    """Process a queue of scraping jobs."""
    pass

def batch_validate_data(data_list: List[ScrapedData]) -> Dict[str, Any]:
    """Validate a batch of scraped data."""
    pass

def aggregate_scraping_results(results: List[ScrapingResult]) -> Dict[str, Any]:
    """Aggregate multiple scraping results."""
    pass

# Error Handling Functions

def handle_scraping_error(
    error: Exception, 
    job_id: str, 
    url: str
) -> Dict[str, Any]:
    """Handle scraping errors and return error info."""
    pass

def retry_failed_scraping(
    failed_data: Dict[str, Any], 
    max_retries: int = 3
) -> Optional[ScrapedData]:
    """Retry failed scraping operations."""
    pass
'''
    
    def generate_export_type_hints(self) -> str:
        """Generate type hints for export functions."""
        return '''
"""
Type hints for data export functions.
Auto-generated by generate_type_hints.py
"""

from typing import Dict, List, Any, Optional, Union, BinaryIO, TextIO
from datetime import datetime
from pathlib import Path
from src.models.pydantic_models import ScrapedData, DataExportRequest

# Export Format Functions

def export_to_csv(
    data: List[Dict[str, Any]], 
    output_path: Union[str, Path], 
    selected_fields: Optional[List[str]] = None
) -> bool:
    """Export data to CSV format."""
    pass

def export_to_json(
    data: List[Dict[str, Any]], 
    output_path: Union[str, Path], 
    include_metadata: bool = True
) -> bool:
    """Export data to JSON format."""
    pass

def export_to_excel(
    data: List[Dict[str, Any]], 
    output_path: Union[str, Path], 
    sheet_name: str = "Scraped Data"
) -> bool:
    """Export data to Excel format."""
    pass

# Data Filtering Functions

def filter_data_by_date_range(
    data: List[Dict[str, Any]], 
    start_date: Optional[datetime], 
    end_date: Optional[datetime]
) -> List[Dict[str, Any]]:
    """Filter data by date range."""
    pass

def filter_data_by_confidence(
    data: List[Dict[str, Any]], 
    min_confidence: float
) -> List[Dict[str, Any]]:
    """Filter data by minimum confidence score."""
    pass

def filter_data_by_job_ids(
    data: List[Dict[str, Any]], 
    job_ids: List[str]
) -> List[Dict[str, Any]]:
    """Filter data by job IDs."""
    pass

def select_export_fields(
    data: List[Dict[str, Any]], 
    selected_fields: List[str]
) -> List[Dict[str, Any]]:
    """Select specific fields for export."""
    pass

# Format Conversion Functions

def convert_nested_objects_to_strings(data: Dict[str, Any]) -> Dict[str, str]:
    """Convert nested objects to string representation for CSV."""
    pass

def flatten_nested_structure(
    data: Dict[str, Any], 
    separator: str = "_"
) -> Dict[str, Any]:
    """Flatten nested dictionary structure."""
    pass

def prepare_data_for_csv(data: List[Dict[str, Any]]) -> List[Dict[str, str]]:
    """Prepare data for CSV export."""
    pass

def prepare_data_for_json(
    data: List[Dict[str, Any]], 
    include_raw_html: bool = False
) -> Dict[str, Any]:
    """Prepare data for JSON export."""
    pass

# Validation Functions

def validate_export_request(request: DataExportRequest) -> Tuple[bool, List[str]]:
    """Validate export request parameters."""
    pass

def validate_export_data(data: List[Dict[str, Any]]) -> Tuple[bool, List[str]]:
    """Validate data before export."""
    pass

def check_export_file_permissions(output_path: Union[str, Path]) -> bool:
    """Check if export file can be written."""
    pass

# Utility Functions

def generate_export_filename(
    format_type: str, 
    job_name: Optional[str] = None, 
    timestamp: Optional[datetime] = None
) -> str:
    """Generate export filename."""
    pass

def calculate_export_size_estimate(data: List[Dict[str, Any]], format_type: str) -> int:
    """Estimate export file size."""
    pass

def create_export_metadata(
    data_count: int, 
    export_params: Dict[str, Any]
) -> Dict[str, Any]:
    """Create metadata for export."""
    pass
'''
    
    def write_type_hints_to_files(self):
        """Write generated type hints to files."""
        # Create type hints directory
        type_hints_dir = project_root / "src" / "types"
        type_hints_dir.mkdir(exist_ok=True)
        
        # Write API type hints
        api_hints_file = type_hints_dir / "api_types.py"
        with open(api_hints_file, 'w') as f:
            f.write(self.generate_api_type_hints())
        
        # Write pipeline type hints
        pipeline_hints_file = type_hints_dir / "pipeline_types.py"
        with open(pipeline_hints_file, 'w') as f:
            f.write(self.generate_pipeline_type_hints())
        
        # Write export type hints
        export_hints_file = type_hints_dir / "export_types.py"
        with open(export_hints_file, 'w') as f:
            f.write(self.generate_export_type_hints())
        
        # Create __init__.py
        init_file = type_hints_dir / "__init__.py"
        with open(init_file, 'w') as f:
            f.write('"""Type hints for AI Web Scraper data transformations."""\n')
        
        print(f"✅ Type hints generated successfully in {type_hints_dir}")
        print("📁 Generated files:")
        print(f"   - {api_hints_file}")
        print(f"   - {pipeline_hints_file}")
        print(f"   - {export_hints_file}")
    
    def analyze_existing_functions(self, file_path: Path) -> List[str]:
        """Analyze existing Python file for functions that need type hints."""
        if not file_path.exists():
            return []
        
        with open(file_path, 'r') as f:
            content = f.read()
        
        try:
            tree = ast.parse(content)
            functions_without_hints = []
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    # Check if function has type hints
                    has_return_annotation = node.returns is not None
                    has_arg_annotations = any(arg.annotation is not None for arg in node.args.args)
                    
                    if not (has_return_annotation or has_arg_annotations):
                        functions_without_hints.append(node.name)
            
            return functions_without_hints
        
        except SyntaxError as e:
            print(f"⚠️  Syntax error in {file_path}: {e}")
            return []
    
    def scan_project_for_missing_hints(self):
        """Scan project for functions missing type hints."""
        print("🔍 Scanning project for functions missing type hints...")
        
        python_files = list(project_root.rglob("*.py"))
        missing_hints_summary = {}
        
        for py_file in python_files:
            # Skip test files and generated files
            if any(skip in str(py_file) for skip in ["/tests/", "/__pycache__/", ".venv", "/types/"]):
                continue
            
            functions_without_hints = self.analyze_existing_functions(py_file)
            if functions_without_hints:
                relative_path = py_file.relative_to(project_root)
                missing_hints_summary[str(relative_path)] = functions_without_hints
        
        if missing_hints_summary:
            print("\n📋 Functions missing type hints:")
            for file_path, functions in missing_hints_summary.items():
                print(f"\n📄 {file_path}:")
                for func in functions:
                    print(f"   - {func}()")
        else:
            print("✅ All functions have type hints!")
        
        return missing_hints_summary


def main():
    """Main function to generate type hints."""
    print("🚀 AI Web Scraper Type Hint Generator")
    print("=" * 50)
    
    generator = TypeHintGenerator()
    
    # Generate and write type hints
    generator.write_type_hints_to_files()
    
    # Scan for missing type hints
    print("\n" + "=" * 50)
    missing_hints = generator.scan_project_for_missing_hints()
    
    if missing_hints:
        print(f"\n⚠️  Found {sum(len(funcs) for funcs in missing_hints.values())} functions missing type hints")
        print("💡 Consider adding type hints to improve code quality and IDE support")
    else:
        print("\n🎉 All functions have proper type hints!")
    
    print("\n✨ Type hint generation completed!")


if __name__ == "__main__":
    main()