#!/usr/bin/env python3
"""
Advanced Scraping Capabilities Demo

This script demonstrates all the advanced scraping features implemented:
- Anti-detection techniques (user agent rotation, stealth mode)
- JavaScript-rendered content handling
- Pagination support and intelligent link following
- Retry logic with exponential backoff and circuit breaker pattern
- Robots.txt respect and ethical scraping practices
"""

import asyncio
import sys
import os
from typing import List

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from src.models.pydantic_models import ScrapingConfig, ScrapingResult
from src.scraper.web_scraper import WebScraper
from src.utils.circuit_breaker import circuit_manager
from src.utils.robots_handler import ethical_enforcer


class AdvancedScrapingDemo:
    """Demonstration of advanced scraping capabilities."""
    
    def __init__(self):
        """Initialize the demo."""
        self.scraper = None
    
    async def demo_anti_detection_techniques(self):
        """Demonstrate anti-detection and stealth capabilities."""
        print("ğŸ•µï¸  Demonstrating Anti-Detection Techniques")
        print("=" * 50)
        
        # Configure scraper with stealth mode
        config = ScrapingConfig(
            use_stealth=True,
            headless=True,
            delay_between_requests=1.0,
            max_retries=2,
            javascript_enabled=True
        )
        
        self.scraper = WebScraper(config)
        
        print("âœ“ Stealth mode enabled")
        print("âœ“ User agent rotation configured")
        print("âœ“ Random delays between requests")
        print("âœ“ Headless browser mode")
        print("âœ“ JavaScript execution enabled")
        
        # Show user agent rotation
        from src.scraper.selenium_driver import SeleniumDriver
        driver = SeleniumDriver(config)
        
        print("\nğŸ”„ User Agent Rotation Demo:")
        for i in range(3):
            ua = driver._rotate_user_agent()
            print(f"  {i+1}. {ua[:60]}...")
        
        print("\nâœ… Anti-detection setup complete!\n")
    
    async def demo_javascript_handling(self):
        """Demonstrate JavaScript content handling."""
        print("ğŸš€ Demonstrating JavaScript Content Handling")
        print("=" * 50)
        
        print("âœ“ Automatic waiting for document.readyState === 'complete'")
        print("âœ“ jQuery AJAX completion detection")
        print("âœ“ Angular HTTP request monitoring")
        print("âœ“ React component loading detection")
        print("âœ“ Custom AJAX request completion checks")
        print("âœ“ Dynamic content loading with configurable waits")
        
        print("\nğŸ“‹ JavaScript Handling Features:")
        print("  â€¢ Waits for DOM ready state")
        print("  â€¢ Detects and waits for AJAX requests")
        print("  â€¢ Handles single-page application (SPA) content")
        print("  â€¢ Configurable wait times for dynamic elements")
        print("  â€¢ Intelligent timeout handling")
        
        print("\nâœ… JavaScript handling configured!\n")
    
    async def demo_pagination_support(self):
        """Demonstrate pagination and link following."""
        print("ğŸ“„ Demonstrating Pagination & Link Following")
        print("=" * 50)
        
        print("ğŸ” Pagination Detection Patterns:")
        pagination_selectors = [
            "a[href*='page']",
            "a[href*='p=']", 
            "a[href*='offset']",
            ".pagination a",
            ".pager a",
            ".page-numbers a",
            "a[rel='next']",
            "a.next",
            ".next-page a"
        ]
        
        for selector in pagination_selectors:
            print(f"  â€¢ {selector}")
        
        print("\nğŸ”— Content Link Detection Patterns:")
        content_selectors = [
            "a[href*='/article/']",
            "a[href*='/post/']",
            "a[href*='/blog/']",
            "a[href*='/news/']",
            "a[href*='/product/']",
            ".content-link a",
            ".article-link a",
            ".post-link a"
        ]
        
        for selector in content_selectors:
            print(f"  â€¢ {selector}")
        
        print("\nâš™ï¸  Link Following Configuration:")
        print("  â€¢ Maximum depth control")
        print("  â€¢ Same-domain restriction")
        print("  â€¢ Duplicate URL prevention")
        print("  â€¢ Intelligent URL normalization")
        print("  â€¢ Rate limiting between pages")
        
        print("\nâœ… Pagination support configured!\n")
    
    async def demo_retry_logic_and_circuit_breaker(self):
        """Demonstrate retry logic and circuit breaker pattern."""
        print("ğŸ”„ Demonstrating Retry Logic & Circuit Breaker")
        print("=" * 50)
        
        # Get circuit breaker stats
        breaker = circuit_manager.get_breaker("web_scraper")
        stats = breaker.get_stats()
        
        print("ğŸ›¡ï¸  Circuit Breaker Configuration:")
        print(f"  â€¢ Failure threshold: 3 failures")
        print(f"  â€¢ Recovery timeout: 30 seconds")
        print(f"  â€¢ Success threshold: 2 successes")
        print(f"  â€¢ Current state: {stats['state']}")
        
        print("\nğŸ” Exponential Backoff Strategy:")
        print("  â€¢ Initial delay: 1.0 seconds")
        print("  â€¢ Backoff multiplier: 2.0x")
        print("  â€¢ Maximum delay: 300 seconds (5 minutes)")
        print("  â€¢ Jitter: Â±10% random variation")
        
        print("\nğŸ“Š Retry Logic Features:")
        print("  â€¢ Automatic retry on network errors")
        print("  â€¢ Exponential backoff between retries")
        print("  â€¢ Circuit breaker prevents cascading failures")
        print("  â€¢ Graceful degradation on persistent failures")
        print("  â€¢ Detailed error logging and metrics")
        
        # Demonstrate backoff calculation
        print("\nğŸ“ˆ Backoff Delay Examples:")
        from src.utils.circuit_breaker import CircuitBreakerConfig
        config = CircuitBreakerConfig(jitter=False)
        
        for i in range(1, 6):
            delay = min(
                config.initial_delay * (config.backoff_multiplier ** (i - 1)),
                config.max_delay
            )
            print(f"  â€¢ Attempt {i}: {delay:.1f} seconds")
        
        print("\nâœ… Retry logic and circuit breaker configured!\n")
    
    async def demo_robots_txt_compliance(self):
        """Demonstrate robots.txt compliance and ethical scraping."""
        print("ğŸ¤– Demonstrating Robots.txt Compliance")
        print("=" * 50)
        
        print("ğŸ“‹ Ethical Scraping Features:")
        print("  â€¢ Automatic robots.txt fetching and parsing")
        print("  â€¢ User-agent specific rule checking")
        print("  â€¢ Crawl-delay respect and enforcement")
        print("  â€¢ Request-rate limiting compliance")
        print("  â€¢ Disallow directive enforcement")
        print("  â€¢ Sitemap discovery and parsing")
        
        print("\nâš–ï¸  Compliance Checks:")
        print("  â€¢ Can-fetch permission verification")
        print("  â€¢ Domain-specific delay enforcement")
        print("  â€¢ Rate limiting between requests")
        print("  â€¢ Respectful crawling practices")
        print("  â€¢ Automatic backoff on 429 responses")
        
        print("\nğŸ• Rate Limiting Strategy:")
        print("  â€¢ Per-domain request tracking")
        print("  â€¢ Configurable default delays")
        print("  â€¢ Robots.txt crawl-delay override")
        print("  â€¢ Request-rate compliance")
        print("  â€¢ Burst prevention mechanisms")
        
        # Show domain stats
        domain_stats = ethical_enforcer.get_domain_stats()
        print(f"\nğŸ“Š Currently tracking {len(domain_stats)} domains")
        
        print("\nâœ… Robots.txt compliance configured!\n")
    
    async def demo_complete_workflow(self):
        """Demonstrate a complete scraping workflow with all features."""
        print("ğŸ¯ Complete Advanced Scraping Workflow")
        print("=" * 50)
        
        # Configure comprehensive scraping
        config = ScrapingConfig(
            # Anti-detection
            use_stealth=True,
            headless=True,
            
            # JavaScript handling
            javascript_enabled=True,
            wait_time=3,
            
            # Pagination and links
            follow_links=True,
            extract_links=True,
            max_depth=2,
            
            # Retry and resilience
            max_retries=3,
            timeout=30,
            
            # Ethical scraping
            respect_robots_txt=True,
            delay_between_requests=2.0
        )
        
        print("ğŸ”§ Workflow Configuration:")
        print(f"  â€¢ Stealth mode: {config.use_stealth}")
        print(f"  â€¢ JavaScript enabled: {config.javascript_enabled}")
        print(f"  â€¢ Follow links: {config.follow_links}")
        print(f"  â€¢ Max retries: {config.max_retries}")
        print(f"  â€¢ Respect robots.txt: {config.respect_robots_txt}")
        print(f"  â€¢ Delay between requests: {config.delay_between_requests}s")
        
        print("\nğŸ“‹ Workflow Steps:")
        print("  1. Check robots.txt permissions")
        print("  2. Initialize stealth browser session")
        print("  3. Navigate with anti-detection measures")
        print("  4. Wait for JavaScript content to load")
        print("  5. Extract content with custom selectors")
        print("  6. Find pagination and content links")
        print("  7. Apply rate limiting and delays")
        print("  8. Retry failed requests with backoff")
        print("  9. Circuit breaker protection")
        print("  10. Clean up resources")
        
        print("\nâœ… Complete workflow ready for execution!\n")
    
    async def show_feature_summary(self):
        """Show a summary of all implemented features."""
        print("ğŸ“‹ Advanced Scraping Features Summary")
        print("=" * 50)
        
        features = {
            "ğŸ•µï¸  Anti-Detection Techniques": [
                "User agent rotation from realistic pool",
                "Stealth browser configuration",
                "Random viewport sizes",
                "Request timing randomization",
                "Advanced Chrome/Firefox stealth options",
                "Navigator property spoofing"
            ],
            "ğŸš€ JavaScript Content Handling": [
                "Document ready state monitoring",
                "AJAX request completion detection",
                "Framework-specific waiting (jQuery, Angular, React)",
                "Dynamic element loading detection",
                "Configurable wait strategies",
                "Timeout handling and fallbacks"
            ],
            "ğŸ“„ Pagination & Link Following": [
                "Intelligent pagination link detection",
                "Content link discovery patterns",
                "Same-domain restriction enforcement",
                "Duplicate URL prevention",
                "Depth-limited crawling",
                "URL normalization and validation"
            ],
            "ğŸ”„ Retry Logic & Circuit Breaker": [
                "Exponential backoff with jitter",
                "Circuit breaker pattern implementation",
                "Failure threshold configuration",
                "Automatic recovery detection",
                "Graceful degradation strategies",
                "Comprehensive error logging"
            ],
            "ğŸ¤– Robots.txt Compliance": [
                "Automatic robots.txt fetching",
                "User-agent specific rule parsing",
                "Crawl-delay enforcement",
                "Request-rate compliance",
                "Disallow directive respect",
                "Ethical scraping practices"
            ]
        }
        
        for category, feature_list in features.items():
            print(f"\n{category}:")
            for feature in feature_list:
                print(f"  âœ“ {feature}")
        
        print(f"\nğŸ‰ Total Features Implemented: {sum(len(f) for f in features.values())}")
        print("\n" + "=" * 50)


async def main():
    """Run the advanced scraping capabilities demonstration."""
    print("ğŸŒŸ Advanced Web Scraping Capabilities Demo")
    print("=" * 60)
    print("This demo showcases all advanced features implemented in Task 4")
    print("=" * 60)
    print()
    
    demo = AdvancedScrapingDemo()
    
    try:
        # Run all demonstrations
        await demo.demo_anti_detection_techniques()
        await demo.demo_javascript_handling()
        await demo.demo_pagination_support()
        await demo.demo_retry_logic_and_circuit_breaker()
        await demo.demo_robots_txt_compliance()
        await demo.demo_complete_workflow()
        await demo.show_feature_summary()
        
        print("ğŸ‰ All advanced scraping capabilities demonstrated successfully!")
        print("\nğŸ’¡ Ready for production use with ethical scraping practices!")
        
    except Exception as e:
        print(f"âŒ Demo failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
    
    finally:
        # Cleanup
        if demo.scraper:
            await demo.scraper.cleanup()
    
    return 0


if __name__ == "__main__":
    # Ensure examples directory exists
    os.makedirs(os.path.dirname(__file__), exist_ok=True)
    
    exit_code = asyncio.run(main())
    sys.exit(exit_code)